{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T01:22:05.273590Z",
     "start_time": "2020-06-24T01:22:03.966729Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy  as np\n",
    "from time import time\n",
    "#train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Feature scaling\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T01:22:05.378710Z",
     "start_time": "2020-06-24T01:22:05.279971Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open('../DistributionDictionary/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "distributions = {}\n",
    "distributions['Benign'] = load_obj(\"Benign_Distribution_set\")\n",
    "distributions['DoS'] = load_obj(\"DoS_Distribution_set\")\n",
    "distributions['PortScan'] = load_obj(\"PortScan_Distribution_set\")\n",
    "distributions['DDoS'] = load_obj(\"DDoS_Distribution_set\")\n",
    "distributions['SSH'] = load_obj(\"SSH_Distribution_set\")\n",
    "distributions['FTP'] = load_obj(\"FTP_Distribution_set\")\n",
    "distributions['Web'] = load_obj(\"Web_Distribution_set\")\n",
    "distributions['Bot'] = load_obj(\"Bot_Distribution_set\")\n",
    "\n",
    "attack_types = 7  #总已知攻击种类\n",
    "\n",
    "#定义difference weight,取值0~1\n",
    "def calculate_dw(distribution_1, distribution_2, feature_index):\n",
    "    count1 = calculate_count(distribution_1, feature_index)\n",
    "    count2 = calculate_count(distribution_2, feature_index)\n",
    "    h = 20\n",
    "    dw = 0\n",
    "    for i in range(h):\n",
    "        start = i*5\n",
    "        end = start + 5\n",
    "        T1 = distribution_1[str(feature_index)][str(start) + '-' + str(end)]/count1\n",
    "        T2 = distribution_2[str(feature_index)][str(start) + '-' + str(end)]/count2\n",
    "        dw += abs(T1 - T2)\n",
    "    return dw/2\n",
    "#用于计算分布字典里的实例数\n",
    "def calculate_count(distribution, feature_index):\n",
    "    h = 20\n",
    "    count = 0\n",
    "    for i in range(h):\n",
    "        start = i*5\n",
    "        end = start + 5\n",
    "        count += distribution[str(feature_index)][str(start) + '-' + str(end)]\n",
    "    return count\n",
    "#定义hdw，hybird difference weight\n",
    "def calculate_hdw(feature_index):\n",
    "    base = 0\n",
    "    for key in distributions:\n",
    "        if(key == \"Benign\"):\n",
    "            continue\n",
    "        dw = calculate_dw(distributions['Benign'], distributions[key], feature_index)\n",
    "        if(dw > base):\n",
    "            base = dw\n",
    "    return base\n",
    "\n",
    "\n",
    "#计算例子在某种标签上的得分，需要提供对应标签的分布统计字典和字典的统计量\n",
    "def get_score(example, distributeObj):    \n",
    "    score = 0\n",
    "    count = calculate_count(distributeObj, 0)  #取哪个特征都一样\n",
    "    for i in range(feature_numbers):\n",
    "\n",
    "        start = int(example[i]/0.05//1)*5   #取200个分组里的分组起始位置\n",
    "        if(start == 100):     #特殊处理当该属性为1.0时溢出的情况\n",
    "            start = 95      \n",
    "        end = start + 5\n",
    "        dataRange = '{0}-{1}'.format(start, end )\n",
    "        base = distributeObj[str(i)][dataRange]/count\n",
    "        if(base == 0):\n",
    "            return -4.0\n",
    "            #return \"attack\"\n",
    "        score += math.log(base) * calculate_hdw(i)  / feature_numbers\n",
    "    return score\n",
    "\n",
    "def get_max(array):\n",
    "    base = 0\n",
    "    for value in array:\n",
    "        if(base < value):\n",
    "            base = value\n",
    "    return base\n",
    "def get_min(array):\n",
    "    base = array[0]\n",
    "    for value in array:\n",
    "        if(base > value):\n",
    "            base = value\n",
    "    return base\n",
    "def get_average(array):\n",
    "    return float(sum(array))/len(array)\n",
    "def get_medium(array):\n",
    "    array.sort()\n",
    "    mid = int(len(array) / 2)\n",
    "    if len(array) % 2 == 0:\n",
    "        median = (array[mid - 1] + array[mid]) / 2.0\n",
    "    else:\n",
    "        median = array[mid]\n",
    "    return median\n",
    "def get_analysis(label_num):\n",
    "    count = 0.0\n",
    "    judge_attack_count = 0.0\n",
    "    Indexs = []\n",
    "    for index, item in enumerate(test_y3):  \n",
    "        if(item == label_num):\n",
    "            Indexs.append(index)\n",
    "            count+=1\n",
    "    Scores = []\n",
    "    for index, item in enumerate(Indexs):\n",
    "        t = get_score(test_x3[item], distributions['Benign'])\n",
    "        if(t == \"attack\"):\n",
    "            judge_attack_count+=1\n",
    "            t = -4.0\n",
    "        Scores.append(t)\n",
    "\n",
    "    print(get_max(Scores))\n",
    "    print(get_min(Scores))\n",
    "    print(get_medium(Scores))\n",
    "    print(get_average(Scores))  \n",
    "    print(judge_attack_count / count) \n",
    "    return Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.915Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(dataroot):\n",
    "    df=pd.read_csv(dataroot,header=0,low_memory=False)   \n",
    "    pd.set_option('mode.use_inf_as_na', True) # convert inf to nan\n",
    "    df['Flow Bytes/s']=df['Flow Bytes/s'].astype('float64')\n",
    "    df[' Flow Packets/s']=df[' Flow Packets/s'].astype('float64')\n",
    "    df['Flow Bytes/s'].fillna(df['Flow Bytes/s'].mean(),inplace=True)\n",
    "    df[' Flow Packets/s'].fillna(df[' Flow Packets/s'].mean(),inplace=True)\n",
    "    return df\n",
    "\n",
    "def split_valid_from_train(train_dataset, valid_size):\n",
    "    # Method 1\n",
    "    train_dataset, valid_dataset, _, _ = train_test_split(train_dataset, train_dataset[' Label'], test_size=valid_size, random_state=None)\n",
    "    # pandas中先重置index再打乱train. 否则只会调整各个行的顺序，而不会改变pandas的index\n",
    "    # 重置\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    # 打乱\n",
    "    indexMask = np.arange(len(train_dataset))\n",
    "    for i in range(10):\n",
    "        np.random.shuffle(indexMask)\n",
    "    train_dataset = train_dataset.iloc[indexMask]\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def shuffle(dataset):  \n",
    "    # 打乱\n",
    "    indexMask = np.arange(len(dataset))\n",
    "    for i in range(10):\n",
    "        np.random.shuffle(indexMask)\n",
    "    dataset = dataset.iloc[indexMask]\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "    return dataset \n",
    "\n",
    "#独热编码。选取distination port统计大于等于10以上的port作为新增维度\n",
    "def create_columns_for_destination_port(train_dataset):\n",
    "    countDictionary = Counter(train_dataset[' Destination Port'])\n",
    "    columns = []\n",
    "    columns.append('DestinationPort_Others')\n",
    "    for key in countDictionary:\n",
    "        if(countDictionary[key] >= 10):\n",
    "            columns.append('DestinationPort_' + str(key))\n",
    "    return columns\n",
    "\n",
    "def one_hot_process_for_destination_port(dataset, columns):\n",
    "    #重置索引\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    #创建空表\n",
    "    DestinationPort_revert = pd.DataFrame(data=np.zeros(shape=(dataset.shape[0],len(columns))), columns = columns)\n",
    "    #填充空表\n",
    "    for i,value in enumerate(dataset[' Destination Port'].values):\n",
    "        name = \"DestinationPort_\" + str(value)\n",
    "        if name in columns:\n",
    "            DestinationPort_revert.loc[i, name] = 1\n",
    "        else:\n",
    "            DestinationPort_revert.loc[i, \"DestinationPort_Others\"] = 1\n",
    "    #与原表连接\n",
    "    dataset = pd.concat([dataset, DestinationPort_revert], axis=1)\n",
    "    #删除原表的destination port列\n",
    "    dataset = dataset.drop([' Destination Port'], axis=1)\n",
    "    return dataset  \n",
    "\n",
    "\n",
    "\n",
    "def labels_separate(dataset):\n",
    "    y_ = dataset[' Label']\n",
    "    temp = dataset\n",
    "    temp.drop([' Label'], axis=1, inplace=True)\n",
    "    x_ = temp.values\n",
    "    return x_, y_\n",
    "\n",
    "def labels_map(label):\n",
    "    if(type(label) == int):      #已是数字，无需处理\n",
    "        return label\n",
    "    if label == 'BENIGN':\n",
    "        return 0\n",
    "    if label == 'DoS':\n",
    "        return 1\n",
    "    if label == 'PortScan':\n",
    "        return 2\n",
    "    if label == 'DDoS':\n",
    "        return 3\n",
    "    if label == 'SSH-Patator':\n",
    "        return 4\n",
    "    if label == 'FTP-Patator':\n",
    "        return 5\n",
    "    if label == 'Web Attack':\n",
    "        return 6\n",
    "    if label == 'Bot':\n",
    "        return 7\n",
    "    if label == 'Infiltration':\n",
    "        return 8\n",
    "    return -1   #-1表示出错\n",
    "def filter_labels(dataset):\n",
    "    dataset[' Label'] = dataset[' Label'].apply(labels_map)\n",
    "    return dataset\n",
    "#特征选取\n",
    "def feature_selection(dataset):\n",
    "    return dataset[[\n",
    "        ' Destination Port', 'Bwd Packet Length Max',' Bwd Packet Length Min', ' Bwd Packet Length Std',\n",
    "        ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Min', ' Fwd IAT Min', ' Bwd Packets/s', ' Min Packet Length',\n",
    "        ' Max Packet Length', ' Init_Win_bytes_backward', ' act_data_pkt_fwd', ' Label'\n",
    "    ]]\n",
    "def feature_selection2(dataset):\n",
    "    return dataset[[' Destination Port', ' Flow Duration', ' Total Fwd Packets', 'Total Length of Fwd Packets', \n",
    "                    ' Total Length of Bwd Packets', ' Fwd Packet Length Std', 'Bwd Packet Length Max', \n",
    "                    ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow IAT Mean', \n",
    "                    ' Flow IAT Max', ' Flow IAT Min', ' Fwd IAT Mean', ' Fwd IAT Max', ' Fwd IAT Min', \n",
    "                    ' Bwd IAT Min', ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s', \n",
    "                    ' Bwd Packets/s', ' Max Packet Length', ' Packet Length Variance', ' ACK Flag Count', \n",
    "                    ' URG Flag Count', ' Average Packet Size', ' Avg Bwd Segment Size', ' Fwd Header Length.1', \n",
    "                    ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' Init_Win_bytes_backward', \n",
    "                    ' act_data_pkt_fwd', ' Label'\n",
    "                   ]]\n",
    "\n",
    "\n",
    "labelList = [\"BENIGN\", \"DoS\", \"PortScan\", \"DDoS\", \"SSH-Patator\", \"FTP-Patator\", \"Web Attack\", \"Bot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.919Z"
    }
   },
   "outputs": [],
   "source": [
    "#DDoS\n",
    "df_1 = read_csv(\"../MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "df_1 = df_1.drop(df_1[df_1[' Label'] == 'BENIGN'].index)\n",
    "df_1 = df_1.sample(n=5000,random_state=1, replace=False)\n",
    "df_1_train, df_1_test = split_valid_from_train(df_1, 0.2)\n",
    "#PortScan\n",
    "df_2 = read_csv(\"../MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
    "df_2 = df_2.drop(df_2[df_2[' Label'] == 'BENIGN'].index)\n",
    "df_2 = df_2.sample(n=5000,random_state=1, replace=False)\n",
    "df_2_train, df_2_test = split_valid_from_train(df_2, 0.2)\n",
    "#Bot\n",
    "df_3 = read_csv(\"../MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
    "df_3 = df_3.drop(df_3[df_3[' Label'] == 'BENIGN'].index)\n",
    "df_3 = df_3.sample(n=5000,random_state=1, replace=True)\n",
    "df_3_train, df_3_test = split_valid_from_train(df_3, 0.2)\n",
    "#Benign\n",
    "df_4 = read_csv(\"../MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_4 = df_4.sample(n=50000,random_state=1, replace=False)\n",
    "df_4_train, df_4_test = split_valid_from_train(df_4, 0.2)\n",
    "#Web attack\n",
    "df_6 = read_csv(\"../MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
    "df_6 = df_6.drop(df_6[df_6[' Label'] == 'BENIGN'].index)\n",
    "df_6 = df_6.sample(n=5000,random_state=1, replace=True)\n",
    "df_6[' Label'] = df_6[' Label'].apply(lambda x: 'Web Attack')\n",
    "df_6_train, df_6_test = split_valid_from_train(df_6, 0.2)\n",
    "#FTP-Patator and SSH-Patator\n",
    "df_7 = read_csv(\"../MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_7 = df_7.drop(df_7[df_7[' Label'] == 'BENIGN'].index)\n",
    "df_7_1 = df_7.drop(df_7[df_7[' Label'] == 'FTP-Patator'].index)\n",
    "df_7_2 = df_7.drop(df_7[df_7[' Label'] == 'SSH-Patator'].index)\n",
    "df_7_1 = df_7_1.sample(n=5000,random_state=1, replace=False)\n",
    "df_7_2 = df_7_2.sample(n=5000,random_state=1, replace=False)\n",
    "df_7_1_train, df_7_1_test = split_valid_from_train(df_7_1, 0.2)\n",
    "df_7_2_train, df_7_2_test = split_valid_from_train(df_7_2, 0.2)\n",
    "#DoS\n",
    "df_8 = read_csv(\"../MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\")\n",
    "df_8 = df_8.drop(df_8[df_8[' Label'] == 'BENIGN'].index)\n",
    "df_8 = df_8.sample(n=5000,random_state=1, replace=False)\n",
    "df_8[' Label'] = df_8[' Label'].apply(lambda x: 'DoS')\n",
    "df_8_train, df_8_test = split_valid_from_train(df_8, 0.2)\n",
    "\n",
    "#整合\n",
    "df_train = df_1_train.append([df_2_train, df_3_train, df_4_train, df_6_train, df_7_1_train,df_7_2_train,df_8_train])\n",
    "df_test = df_1_test.append([df_2_test, df_3_test, df_4_test, df_6_test, df_7_1_test,df_7_2_test,df_8_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.922Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_train.loc[:,' Label'].value_counts())\n",
    "print(df_test.shape)\n",
    "print(df_test.loc[:,' Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.925Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#数理统计方法，按5%区间进行数理统计\n",
    "def statistics(dataset, feature_index):\n",
    "    intervals = {'{0}-{1}'.format(5 * x, 5 * (x+1) ): 0 for x in range(20)}\n",
    "    for _ in dataset[:,feature_index]:\n",
    "        for interval in intervals:\n",
    "            start, end = tuple(interval.split('-'))\n",
    "            if int(start) <= _*100 <= int(end):\n",
    "                intervals[interval] += 1\n",
    "    return intervals\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('../DistributionDictionary/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.927Z"
    }
   },
   "outputs": [],
   "source": [
    "#整合后再乱序一下\n",
    "df_train = shuffle(df_train)\n",
    "#标签的字符转化成数字\n",
    "df_train = filter_labels(df_train)\n",
    "df_test = filter_labels(df_test)\n",
    "#特征选择\n",
    "df_train = feature_selection(df_train)\n",
    "df_test = feature_selection(df_test)\n",
    "#对destination port进行独热编码\n",
    "columns = create_columns_for_destination_port(df_train)\n",
    "df_train = one_hot_process_for_destination_port(df_train, columns)\n",
    "df_test = one_hot_process_for_destination_port(df_test, columns)\n",
    "#分离标签\n",
    "df_train_X, df_train_y = labels_separate(df_train)\n",
    "df_test_X, df_test_y = labels_separate(df_test)\n",
    "#Normalization\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "df_train_X = scaler.fit_transform(df_train_X)\n",
    "df_test_X = scaler.fit_transform(df_test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.930Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_X = create_distribution_score(df_train_X)\n",
    "df_test_X = create_distribution_score(df_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.934Z"
    }
   },
   "outputs": [],
   "source": [
    "print(columns)\n",
    "print(len(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.937Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_train_X.shape)\n",
    "print(df_train_y.shape)\n",
    "print(df_test_X.shape)\n",
    "print(df_test_y.shape)\n",
    "print(df_train_X[0])\n",
    "print(df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.941Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_numbers = 12   #选前12个特征\n",
    "distribution_feature = [    #产生的新特征\n",
    "    'Benign_Score', 'DoS_Score',\n",
    "    'PortScan_Score', 'DDoS_Score', 'SSH_Score',\n",
    "    'FTP_Score', ' Web_Score', 'Bot_Score'\n",
    "]\n",
    "def create_distribution_score(dataset):\n",
    "    #创建空表\n",
    "    Score_revert = pd.DataFrame(\n",
    "        data=np.zeros(\n",
    "            shape=(\n",
    "                dataset.shape[0],\n",
    "                len(distribution_feature)\n",
    "            )\n",
    "        ),columns = distribution_feature\n",
    "    )\n",
    "    #填充空表\n",
    "    for i,item in enumerate(dataset):\n",
    "        score = []\n",
    "        minIndex = 0\n",
    "        Score_revert.loc[i, 'Benign_Score'] = get_score(item, distributions['Benign'])\n",
    "        Score_revert.loc[i, 'DoS_Score'] = get_score(item, distributions['DoS'])\n",
    "        Score_revert.loc[i, 'PortScan_Score'] = get_score(item, distributions['PortScan'])\n",
    "        Score_revert.loc[i, 'DDoS_Score'] = get_score(item, distributions['DDoS'])\n",
    "        Score_revert.loc[i, 'SSH_Score'] = get_score(item, distributions['SSH'])\n",
    "        Score_revert.loc[i, 'FTP_Score'] = get_score(item, distributions['FTP'])\n",
    "        Score_revert.loc[i, 'Web_Score'] = get_score(item, distributions['Web'])\n",
    "        Score_revert.loc[i, 'Bot_Score'] = get_score(item, distributions['Bot'])\n",
    "\n",
    "#     #与原表连接\n",
    "#     dataset = pd.concat([dataset, DestinationPort_revert], axis=1)\n",
    "    dataset = np.hstack((dataset,Score_revert.values))\n",
    "    return dataset  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.946Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#超参数设置\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "activation = tf.nn.relu         #非输出层下的激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.952Z"
    }
   },
   "outputs": [],
   "source": [
    "#用于记录训练中每个batch的loss\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_losses_per_batch = []\n",
    "        self.train_losses_per_epoch = []\n",
    "        self.valid_losses_per_epoch = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_losses_per_batch.append(logs.get('loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.train_losses_per_epoch.append(logs.get('loss'))\n",
    "        self.valid_losses_per_epoch.append(logs.get('val_loss')*4)   #验证集由于只有1/4的训练集大小所以损失要乘以4\n",
    "#用于记录训练中每个epoch的召回率，精确度以及f1 score\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        predictions = self.model.predict(df_test_X)\n",
    "        val_predict = np.argmax(predictions, axis=1)     #把独热编码转化成数字\n",
    "        val_targ = df_test_y\n",
    "        _val_recall = recall_score(val_targ, val_predict, average='macro')\n",
    "        _val_precision = precision_score(val_targ, val_predict, average='macro')\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='macro')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        return       \n",
    "#回调：模型跑完epoch后选取最好的epoch模型保存，选取标准为验证集损失最小的那一个\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(      \n",
    "    \"./Test7_model.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    period=1\n",
    ")\n",
    "def simple_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(df_train_X.shape[1], activation=activation, input_shape=(df_train_X.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=activation),\n",
    "        BatchNormalization(),\n",
    "        keras.layers.Dense(8,activation=tf.nn.softmax)\n",
    "    ])\n",
    "    model.compile(optimizer =tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "    history = LossHistory()\n",
    "    metrics = Metrics()\n",
    "    model.fit(\n",
    "        df_train_X,df_train_y,\n",
    "        validation_data=[df_test_X, df_test_y],\n",
    "        batch_size=batch_size,epochs=epochs,\n",
    "        callbacks=[history, metrics, checkpoint]\n",
    "    )   \n",
    "    return model,history,metrics\n",
    "model,history,metrics = simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.957Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.960Z"
    }
   },
   "outputs": [],
   "source": [
    "#绘制训练集在batch下的损失变化\n",
    "plt.title('The Cost with batchs runs',fontsize=30)\n",
    "plt.xlabel('batchs',fontsize=20)\n",
    "plt.ylabel('Cost',fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.plot(history.train_losses_per_batch)\n",
    "plt.gcf().set_size_inches(15,4)\n",
    "plt.show()\n",
    "#绘制训练集与验证集在epoch下的损失比较\n",
    "plt.title('The Cost with epoches runs',fontsize=30)\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('Cost',fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.plot(history.train_losses_per_epoch, '-o', label='train')\n",
    "plt.plot(history.valid_losses_per_epoch, '-o', label='valid')\n",
    "plt.legend(fontsize=30,loc='upper right')\n",
    "plt.gcf().set_size_inches(15,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.963Z"
    }
   },
   "outputs": [],
   "source": [
    "#选取验证集准确率最高的模型\n",
    "model = keras.models.load_model('./Test7_model.h5') \n",
    "model.compile(optimizer =tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "#测试集的正确率\n",
    "def use_evaluate_test():\n",
    "    test_loss,test_acc = model.evaluate(df_test_X,df_test_y)\n",
    "    print('Test accuracy:', test_acc)\n",
    "    return \n",
    "use_evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T01:22:03.967Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(df_test_X)\n",
    "print(classification_report(df_test_y, np.argmax(predictions, axis=1) , target_names=labelList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
