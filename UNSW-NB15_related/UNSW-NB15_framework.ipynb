{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  #忽略烦人的警告\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)   #忽略烦人的警告\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle   #对象存储用\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "dataroot_1 = \"./UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv\"\n",
    "dataroot_2 = \"./UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_testing-set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.143Z"
    }
   },
   "outputs": [],
   "source": [
    "labelList = [\n",
    "    \"Normal\", \"Fuzzers\", \"Analysis\", \"Backdoor\", \"DoS\", \"Exploits\", \n",
    "    \"Generic\", \"Reconnaissance\", \"Shellcode\", \"Worms\"\n",
    "]\n",
    "distribution_feature = [    #产生的新特征\n",
    "    \"Normal_Score\", \"Fuzzers_Score\", \"Analysis_Score\", \"Backdoor_Score\", \"DoS_Score\", \"Exploits_Score\", \n",
    "    \"Generic_Score\", \"Reconnaissance_Score\", \"Shellcode_Score\", \"Worms_Score\"\n",
    "]\n",
    "def load_obj(name):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "distributions = {}\n",
    "distributions['Normal'] = load_obj(\"normal_Distribution_set\")\n",
    "distributions['Fuzzers'] = load_obj(\"Fuzzers_Distribution_set\")\n",
    "distributions['Analysis'] = load_obj(\"Analysis_Distribution_set\")\n",
    "distributions['Backdoor'] = load_obj(\"Backdoor_Distribution_set\")\n",
    "distributions['DoS'] = load_obj(\"DoS_Distribution_set\")\n",
    "distributions['Exploits'] = load_obj(\"Exploits_Distribution_set\")\n",
    "distributions['Generic'] = load_obj(\"Generic_Distribution_set\")\n",
    "distributions['Reconnaissance'] = load_obj(\"Reconnaissance_Distribution_set\")\n",
    "distributions['Shellcode'] = load_obj(\"Shellcode_Distribution_set\")\n",
    "distributions['Worms'] = load_obj(\"Worms_Distribution_set\")\n",
    "\n",
    "\n",
    "attack_types = 9  #总已知攻击种类\n",
    "feature_numbers = 23   #用于处理DSM的特征，位列前35的特征\n",
    "\n",
    "\n",
    "#定义difference weight,取值0~1\n",
    "def calculate_dw(distribution_1, distribution_2, feature_index):\n",
    "    count1 = calculate_count(distribution_1, feature_index)\n",
    "    count2 = calculate_count(distribution_2, feature_index)\n",
    "    h = 20\n",
    "    dw = 0\n",
    "    for i in range(h):\n",
    "        start = i*5\n",
    "        end = start + 5\n",
    "        T1 = distribution_1[str(feature_index)][str(start) + '-' + str(end)]/count1\n",
    "        T2 = distribution_2[str(feature_index)][str(start) + '-' + str(end)]/count2\n",
    "        dw += abs(T1 - T2)\n",
    "    return dw/2\n",
    "#用于计算分布字典里的实例数\n",
    "def calculate_count(distribution, feature_index):\n",
    "    h = 20\n",
    "    count = 0\n",
    "    for i in range(h):\n",
    "        start = i*5\n",
    "        end = start + 5\n",
    "        count += distribution[str(feature_index)][str(start) + '-' + str(end)]\n",
    "    return count\n",
    "#定义hdw，hybird difference weight  第二个参数为：hdw应用的对应得分公式的对应的哪种字典\n",
    "def calculate_hdw(feature_index, label):\n",
    "    base = 0\n",
    "    for key in distributions:\n",
    "        if(key == label):\n",
    "            continue\n",
    "        dw = calculate_dw(distributions[label], distributions[key], feature_index)\n",
    "        if(dw > base):\n",
    "            base = dw\n",
    "    return base\n",
    "\n",
    "\n",
    "#计算例子在某种标签上的得分，需要提供对应标签, 这个标签的的分布统计字典\n",
    "def get_score(example, distributeObj, label):    \n",
    "    score = 0\n",
    "    count = calculate_count(distributeObj, 0)  #取哪个特征都一样\n",
    "    minBase = 1\n",
    "    baseArr = []\n",
    "    for i in range(feature_numbers):\n",
    "\n",
    "        start = int(example[i]/0.05//1)*5   #取200个分组里的分组起始位置\n",
    "        if(start == 100):     #特殊处理当该属性为1.0时溢出的情况\n",
    "            start = 95      \n",
    "        end = start + 5\n",
    "        dataRange = '{0}-{1}'.format(start, end )\n",
    "        base = distributeObj[str(i)][dataRange]/count\n",
    "        if(base != 0 and base < minBase):\n",
    "            minBase = base\n",
    "        baseArr.append(base)\n",
    "        \n",
    "    for i in range(feature_numbers):\n",
    "        if(baseArr[i] != 0):\n",
    "            score += math.log(baseArr[i]) * calculate_hdw(i, label)  / feature_numbers\n",
    "        else:\n",
    "            score += math.log(minBase) * calculate_hdw(i, label)  / feature_numbers\n",
    "            \n",
    "    return score\n",
    "\n",
    "def get_score_threshold(dataset_x, dataset_y):\n",
    "    #10个分数数组，表示在这个label字典下label实例的得分情况\n",
    "    Scores = {\n",
    "        \"Normal\" : [],\n",
    "        \"Fuzzers\" : [],\n",
    "        \"Analysis\" : [],\n",
    "        \"Backdoor\" : [],\n",
    "        \"DoS\" : [], \n",
    "        \"Exploits\" : [],\n",
    "        \"Generic\" : [],\n",
    "        \"Reconnaissance\" : [],\n",
    "        \"Shellcode\" : [],\n",
    "        \"Worms\" : []\n",
    "    }\n",
    "    for i,item in enumerate(dataset_x):\n",
    "        if(dataset_y.values[i].astype('int') == 0):\n",
    "            \n",
    "            Scores[\"Normal\"].append(get_score(item, distributions['Normal'], 'Normal'))\n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 1):\n",
    "            \n",
    "            Scores[\"Fuzzers\"].append(get_score(item, distributions['Fuzzers'], 'Fuzzers'))\n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 2):\n",
    "            \n",
    "            Scores[\"Analysis\"].append(get_score(item, distributions['Analysis'], 'Analysis'))\n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 3):\n",
    "            \n",
    "            Scores[\"Backdoor\"].append(get_score(item, distributions['Backdoor'], 'Backdoor'))\n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 4):\n",
    "            \n",
    "            Scores[\"DoS\"].append(get_score(item, distributions['DoS'], 'DoS'))\n",
    "                        \n",
    "        elif(dataset_y.values[i].astype('int') == 5):\n",
    "            \n",
    "            Scores[\"Exploits\"].append(get_score(item, distributions['Exploits'], 'Exploits'))\n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 6):\n",
    "            \n",
    "            Scores[\"Generic\"].append(get_score(item, distributions['Generic'], 'Generic'))\n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 7):\n",
    "            \n",
    "            Scores[\"Reconnaissance\"].append(get_score(item, distributions['Reconnaissance'], 'Reconnaissance'))\n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 8):\n",
    "            \n",
    "            Scores[\"Shellcode\"].append(get_score(item, distributions['Shellcode'], 'Shellcode')) \n",
    "            \n",
    "        elif(dataset_y.values[i].astype('int') == 9):\n",
    "            \n",
    "            Scores[\"Worms\"].append(get_score(item, distributions['Worms'], 'Worms'))\n",
    "            \n",
    "    thresholds = []\n",
    "    \n",
    "    for item in Scores.values():\n",
    "        item.sort()\n",
    "        thresholds.append(item[len(item)//10])    #取”将样例分成1：9“的threshold\n",
    "    return thresholds\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def fill_score(i, item, Score_revert, label, label_num):\n",
    "    #计算得分\n",
    "    score = get_score(item, distributions[label], label)\n",
    "    #score-normalization\n",
    "    score_norm = sigmoid(score - thresholds[label_num])\n",
    "    #放入要拼接的表\n",
    "    Score_revert.loc[i, (label + '_Score')] = score_norm\n",
    "    return\n",
    "\n",
    "def create_distribution_score(dataset_x, thresholds):\n",
    "    #创建空表\n",
    "    Score_revert = pd.DataFrame(\n",
    "        data=np.zeros(\n",
    "            shape=(\n",
    "                dataset_x.shape[0],\n",
    "                len(distribution_feature)\n",
    "            )\n",
    "        ),columns = distribution_feature\n",
    "    )\n",
    "    #填充空表\n",
    "    for i,item in enumerate(dataset_x):\n",
    "        fill_score(i, item, Score_revert, 'Normal', 0)\n",
    "        fill_score(i, item, Score_revert, 'Fuzzers', 1)\n",
    "        fill_score(i, item, Score_revert, 'Analysis', 2)\n",
    "        fill_score(i, item, Score_revert, 'Backdoor', 3)\n",
    "        fill_score(i, item, Score_revert, 'DoS', 4)        \n",
    "        fill_score(i, item, Score_revert, 'Exploits', 5)\n",
    "        fill_score(i, item, Score_revert, 'Generic', 6)\n",
    "        fill_score(i, item, Score_revert, 'Reconnaissance', 7)\n",
    "        fill_score(i, item, Score_revert, 'Shellcode', 8)\n",
    "        fill_score(i, item, Score_revert, 'Worms', 9)\n",
    "            \n",
    "    dataset_x = np.hstack((dataset_x,Score_revert.values))\n",
    "    return dataset_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.148Z"
    }
   },
   "outputs": [],
   "source": [
    "#选择需要进行创建字典的特征。实际有\n",
    "def feature_selection(dataset):\n",
    "    return dataset[[\n",
    "        'dur', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', \n",
    "        'sloss', 'sinpkt', 'sjit', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', \n",
    "        'ct_srv_src', 'ct_state_ttl', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', \n",
    "        'ct_dst_src_ltm', 'ct_srv_dst', 'proto_udp', 'service_-', 'service_dns', 'attack_cat'\n",
    "    ]]\n",
    "\n",
    "def split_valid_from_train(train_dataset, valid_size):\n",
    "    # Method 1\n",
    "    train_dataset, valid_dataset, _, _ = train_test_split(train_dataset, train_dataset['attack_cat'], test_size=valid_size, random_state=None)\n",
    "    # pandas中先重置index再打乱train. 否则只会调整各个行的顺序，而不会改变pandas的index\n",
    "    # 重置\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    # 打乱\n",
    "    indexMask = np.arange(len(train_dataset))\n",
    "    for i in range(10):\n",
    "        np.random.shuffle(indexMask)\n",
    "    train_dataset = train_dataset.iloc[indexMask]\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def shuffle(dataset):  \n",
    "    # 打乱\n",
    "    indexMask = np.arange(len(dataset))\n",
    "    for i in range(10):\n",
    "        np.random.shuffle(indexMask)\n",
    "    dataset = dataset.iloc[indexMask]\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "    return dataset \n",
    "\n",
    "def filter_useless_feature(dataset):\n",
    "    dataset = dataset.drop(['label'], axis=1)    #两个特征没用\n",
    "    dataset = dataset.drop(['id'], axis=1)\n",
    "    return dataset\n",
    "\n",
    "def filter_labels(dataset):\n",
    "    dataset['attack_cat'] = dataset['attack_cat'].apply(labels_map)\n",
    "    return dataset\n",
    "\n",
    "def labels_map(label):\n",
    "    if(type(label) == int):      #已是数字，无需处理\n",
    "        return label\n",
    "    \n",
    "    if label == 'Normal':\n",
    "        return 0\n",
    "    if label == 'Fuzzers':\n",
    "        return 1\n",
    "    if label == 'Analysis':\n",
    "        return 2\n",
    "    if label == 'Backdoor':\n",
    "        return 3\n",
    "    if label == 'DoS':\n",
    "        return 4\n",
    "    if label == 'Exploits':\n",
    "        return 5\n",
    "    if label == 'Generic':\n",
    "        return 6\n",
    "    if label == 'Reconnaissance':\n",
    "        return 7\n",
    "    if label == 'Shellcode':\n",
    "        return 8\n",
    "    if label == 'Worms':\n",
    "        return 9\n",
    "    return -1\n",
    "\n",
    "def labels_separate(dataset):\n",
    "    y_ = dataset['attack_cat'].astype('int')\n",
    "    temp = dataset\n",
    "    temp.drop(['attack_cat'], axis=1, inplace=True)\n",
    "    x_ = temp.values\n",
    "    return x_, y_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.151Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_process(dataset):\n",
    "    # 独热编码 state\n",
    "    state_dummies = pd.get_dummies(dataset['state'], prefix='state')\n",
    "    dataset = pd.concat([dataset, state_dummies], axis=1)\n",
    "    dataset = dataset.drop(['state'], axis=1)\n",
    "    # 独热编码 proto\n",
    "    proto_dummies = pd.get_dummies(dataset['proto'], prefix='proto')\n",
    "    dataset = pd.concat([dataset, proto_dummies], axis=1)\n",
    "    dataset = dataset.drop(['proto'], axis=1)\n",
    "    # 独热编码 service     \n",
    "    service_dummies = pd.get_dummies(dataset['service'], prefix='service')\n",
    "    dataset = pd.concat([dataset, service_dummies], axis=1)\n",
    "    dataset = dataset.drop(['service'], axis=1)\n",
    "    return dataset\n",
    "#归一化整个数据集，注意要存储归一化的参数x_avg,x_max,x_min以便后面真实环境预处理需要\n",
    "def normalization(dataset):\n",
    "    epsilon = 0.0001   #防止除数为0\n",
    "    dataset = dataset.astype('float')\n",
    "    target_features = ['dur','spkts', 'dpkts', 'sbytes',\n",
    "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
    "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
    "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
    "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
    "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
    "       'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
    "       'ct_srv_dst'\n",
    "    ]\n",
    "    norm_params = pd.DataFrame(np.zeros([3, len(target_features)]), columns=target_features)  #存储归一化参数\n",
    "    \n",
    "    for feature in dataset.columns.values.tolist():    #对列（特征名）进行遍历\n",
    "        if feature in target_features:\n",
    "            x = dataset[feature]\n",
    "            x_avg = dataset[feature].mean()\n",
    "            x_max = np.max(dataset[feature])\n",
    "            x_min = np.min(dataset[feature])\n",
    "\n",
    "            norm_params[feature] = [x_avg, x_max, x_min]\n",
    "            dataset[feature] = (x - x_min) / (x_max - x_min + epsilon)\n",
    "    return dataset,norm_params\n",
    "    \n",
    "def combine_dataset(trainDF, testDF):\n",
    "    all = pd.concat([trainDF, testDF], axis=0)\n",
    "    return all, (trainDF.shape[0], testDF.shape[0])\n",
    "\n",
    "def separate_dataset(comb, num_comb):  \n",
    "    train_dataset_size , test_dataset_size = num_comb\n",
    "    trainDF = comb.iloc[:train_dataset_size]\n",
    "    testDF = comb.iloc[train_dataset_size: (train_dataset_size + test_dataset_size)]\n",
    "    return trainDF, testDF\n",
    "\n",
    "   \n",
    "def data_process_full(trainDF, testDF):\n",
    "    all, num_comb = combine_dataset(trainDF, testDF)\n",
    "    all = one_hot_process(all)\n",
    "    all,norm_params = normalization(all)\n",
    "    trainDF,testDF = separate_dataset(all, num_comb)\n",
    "    return trainDF,testDF,norm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.156Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_csv(dataroot_1,header=0,low_memory=False)  \n",
    "df_train_2 = pd.read_csv(dataroot_2,header=0,low_memory=False)  \n",
    "dataset = df_train_1.append([df_train_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.160Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = filter_useless_feature(dataset)\n",
    "dataset = filter_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.163Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_df = dataset.loc[dataset['attack_cat'] == 0]\n",
    "normal_df = normal_df.sample(n=90000,random_state=1, replace=False)\n",
    "df_normal_train, df_normal_test = split_valid_from_train(normal_df, 0.2)\n",
    "\n",
    "Fuzzers_df = dataset.loc[dataset['attack_cat'] == 1]\n",
    "Fuzzers_df = Fuzzers_df.sample(n=5000,random_state=1, replace=False)\n",
    "df_Fuzzers_train, df_Fuzzers_test = split_valid_from_train(Fuzzers_df, 0.2)\n",
    "\n",
    "Analysis_df = dataset.loc[dataset['attack_cat'] == 2]\n",
    "Analysis_df = Analysis_df.sample(n=5000,random_state=1, replace=True)\n",
    "df_Analysis_train, df_Analysis_test = split_valid_from_train(Analysis_df, 0.2)\n",
    "\n",
    "Backdoor_df = dataset.loc[dataset['attack_cat'] == 3]\n",
    "Backdoor_df = Backdoor_df.sample(n=5000,random_state=1, replace=True)\n",
    "df_Backdoor_train, df_Backdoor_test = split_valid_from_train(Backdoor_df, 0.2)\n",
    "\n",
    "DoS_df = dataset.loc[dataset['attack_cat'] == 4]\n",
    "DoS_df = DoS_df.sample(n=5000,random_state=1, replace=False)\n",
    "df_DoS_train, df_DoS_test = split_valid_from_train(DoS_df, 0.2)\n",
    "\n",
    "Exploits_df = dataset.loc[dataset['attack_cat'] == 5]\n",
    "Exploits_df = Exploits_df.sample(n=5000,random_state=1, replace=False)\n",
    "df_Exploits_train, df_Exploits_test = split_valid_from_train(Exploits_df, 0.2)\n",
    "\n",
    "Generic_df = dataset.loc[dataset['attack_cat'] == 6]\n",
    "Generic_df = Generic_df.sample(n=5000,random_state=1, replace=False)\n",
    "df_Generic_train, df_Generic_test = split_valid_from_train(Generic_df, 0.2)\n",
    "\n",
    "Reconnaissance_df = dataset.loc[dataset['attack_cat'] == 7]\n",
    "Reconnaissance_df = Reconnaissance_df.sample(n=5000,random_state=1, replace=False)\n",
    "df_Reconnaissance_train, df_Reconnaissance_test = split_valid_from_train(Reconnaissance_df, 0.2)\n",
    "\n",
    "Shellcode_df = dataset.loc[dataset['attack_cat'] == 8]\n",
    "Shellcode_df = Shellcode_df.sample(n=5000,random_state=1, replace=True)\n",
    "df_Shellcode_train, df_Shellcode_test = split_valid_from_train(Shellcode_df, 0.2)\n",
    "\n",
    "Worms_df = dataset.loc[dataset['attack_cat'] == 9]\n",
    "Worms_df = Worms_df.sample(n=5000,random_state=1, replace=True)\n",
    "df_Worms_train, df_Worms_test = split_valid_from_train(Worms_df, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.167Z"
    }
   },
   "outputs": [],
   "source": [
    "#整合\n",
    "df_train = df_normal_train.append([df_Fuzzers_train, df_Analysis_train, df_Backdoor_train, df_DoS_train, df_Exploits_train, df_Generic_train, df_Reconnaissance_train, df_Shellcode_train, df_Worms_train])\n",
    "df_test = df_normal_test.append([df_Fuzzers_test, df_Analysis_test, df_Backdoor_test, df_DoS_test, df_Exploits_test, df_Generic_test, df_Reconnaissance_test, df_Shellcode_test, df_Worms_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.171Z"
    }
   },
   "outputs": [],
   "source": [
    "#独热编码和正则化\n",
    "df_train, df_test, norm_params = data_process_full(\n",
    "    df_train, df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.174Z"
    }
   },
   "outputs": [],
   "source": [
    "#特征选择\n",
    "df_train = feature_selection(df_train)\n",
    "df_test = feature_selection(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.177Z"
    }
   },
   "outputs": [],
   "source": [
    "#分离标签\n",
    "df_train_X, df_train_y = labels_separate(df_train)\n",
    "df_test_X, df_test_y = labels_separate(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.181Z"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = get_score_threshold(df_train_X, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.186Z"
    }
   },
   "outputs": [],
   "source": [
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.189Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_X = create_distribution_score(df_train_X, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.193Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_X = create_distribution_score(df_test_X, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.196Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_y = df_train_y.astype('int')\n",
    "df_test_y = df_test_y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.202Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_train_X.shape)\n",
    "print(df_test_X.shape)\n",
    "print(df_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.205Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#超参数设置\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "activation = tf.nn.relu         #非输出层下的激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.212Z"
    }
   },
   "outputs": [],
   "source": [
    "#用于记录训练中每个batch的loss\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_losses_per_batch = []\n",
    "        self.train_losses_per_epoch = []\n",
    "        self.valid_losses_per_epoch = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_losses_per_batch.append(logs.get('loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.train_losses_per_epoch.append(logs.get('loss'))\n",
    "        self.valid_losses_per_epoch.append(logs.get('val_loss')*4)   #验证集由于只有1/4的训练集大小所以损失要乘以4\n",
    "#用于记录训练中每个epoch的召回率，精确度以及f1 score\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        predictions = self.model.predict(df_test_X)\n",
    "        val_predict = np.argmax(predictions, axis=1)     #把独热编码转化成数字\n",
    "        val_targ = df_test_y\n",
    "        _val_recall = recall_score(val_targ, val_predict, average='macro')\n",
    "        _val_precision = precision_score(val_targ, val_predict, average='macro')\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='macro')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        return       \n",
    "#回调：模型跑完epoch后选取最好的epoch模型保存，选取标准为验证集损失最小的那一个\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(      \n",
    "    \"./UNSW-NB15_model.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    period=1\n",
    ")\n",
    "def simple_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(df_train_X.shape[1], activation=activation, input_shape=(df_train_X.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        keras.layers.Dense(128, activation=activation),\n",
    "        BatchNormalization(),\n",
    "        keras.layers.Dense(64, activation=activation),\n",
    "        BatchNormalization(),        \n",
    "        keras.layers.Dense(32, activation=activation),\n",
    "        BatchNormalization(),\n",
    "        keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "    ])\n",
    "    model.compile(optimizer =tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "    history = LossHistory()\n",
    "    metrics = Metrics()\n",
    "    model.fit(\n",
    "        df_train_X,df_train_y,\n",
    "        validation_data=[df_test_X, df_test_y],\n",
    "        batch_size=batch_size,epochs=epochs,\n",
    "        callbacks=[history, metrics, checkpoint]\n",
    "    )   \n",
    "    return model,history,metrics\n",
    "model,history,metrics = simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.215Z"
    }
   },
   "outputs": [],
   "source": [
    "#绘制训练集在batch下的损失变化\n",
    "plt.title('The Cost with batchs runs',fontsize=30)\n",
    "plt.xlabel('batchs',fontsize=20)\n",
    "plt.ylabel('Cost',fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.plot(history.train_losses_per_batch)\n",
    "plt.gcf().set_size_inches(15,4)\n",
    "plt.show()\n",
    "#绘制训练集与验证集在epoch下的损失比较\n",
    "plt.title('The Cost with epoches runs',fontsize=30)\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('Cost',fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.plot(history.train_losses_per_epoch, '-o', label='train')\n",
    "plt.plot(history.valid_losses_per_epoch, '-o', label='valid')\n",
    "plt.legend(fontsize=30,loc='upper right')\n",
    "plt.gcf().set_size_inches(15,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.218Z"
    }
   },
   "outputs": [],
   "source": [
    "#选取验证集准确率最高的模型\n",
    "model = keras.models.load_model('./UNSW-NB15_model.h5') \n",
    "model.compile(optimizer =tf.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "#测试集的正确率\n",
    "def use_evaluate_test():\n",
    "    test_loss,test_acc = model.evaluate(df_test_X,df_test_y)\n",
    "    print('Test accuracy:', test_acc)\n",
    "    return \n",
    "use_evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.226Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(df_test_X)\n",
    "print(classification_report(df_test_y, np.argmax(predictions, axis=1) , target_names=labelList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.229Z"
    }
   },
   "outputs": [],
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.233Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# 运行程序\n",
    "clf = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=3)\n",
    "dt_clf = clf.fit(df_train_X, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.236Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = dt_clf.predict(df_test_X)\n",
    "print(predictions)\n",
    "print(accuracy_score(df_test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.242Z"
    }
   },
   "outputs": [],
   "source": [
    "score = 0\n",
    "for i in range(5):\n",
    "    clf = DecisionTreeClassifier(criterion='gini', min_samples_leaf=3)\n",
    "    clf.fit(df_train_X, df_train_y)\n",
    "    y_pred = clf.predict(df_test_X)\n",
    "    result = clf.score(df_test_X, df_test_y)\n",
    "    if(score < result):\n",
    "        score = result\n",
    "        print(score)\n",
    "        print(classification_report(df_test_y, y_pred, target_names=labelList))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.245Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "score = 0\n",
    "for i in range(5):\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(df_train_X, df_train_y)\n",
    "    y_pred = rfc.predict(df_test_X)\n",
    "    result = rfc.score(df_test_X, df_test_y)\n",
    "    if(score < result):\n",
    "        score = result\n",
    "        print(score)\n",
    "        print(classification_report(df_test_y, y_pred, target_names=labelList))\n",
    "        \n",
    "#测试集的正确率\n",
    "#测试集的各个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.249Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "score = 0\n",
    "for i in range(5):\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(df_train_X, df_train_y)\n",
    "    y_pred = xgb.predict(df_test_X)\n",
    "    result = xgb.score(df_test_X, df_test_y)\n",
    "    if(score < result):\n",
    "        score = result\n",
    "        print(score)\n",
    "        print(classification_report(df_test_y, y_pred, target_names=labelList))\n",
    "        \n",
    "#测试集的正确率\n",
    "#测试集的各个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.252Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(df_train_X, df_train_y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.255Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = clf.predict(df_test_X)\n",
    "print(predictions)\n",
    "print(accuracy_score(df_test_y, predictions))\n",
    "print(classification_report(df_test_y, predictions , target_names=labelList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T02:27:31.257Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# score = 0\n",
    "# for i in range(5):\n",
    "#     rfc = RandomForestClassifier()\n",
    "#     rfc.fit(T_train_X, df_train_y)\n",
    "#     y_pred = rfc.predict(T_test_X)\n",
    "#     result = rfc.score(T_test_X, df_test_y)\n",
    "#     if(score < result):\n",
    "#         score = result\n",
    "#         print(score)\n",
    "#         print(classification_report(df_test_y, y_pred, target_names=labelList))\n",
    "        \n",
    "# #测试集的正确率\n",
    "# #测试集的各个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
